{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c6497-d4e1-44b1-9714-e71601ead05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to our processed data\n",
    "DATA_FILE = '../data/processed/OAI_tri_modal_cohort_knee_level.parquet'\n",
    "\n",
    "# Load the master cohort\n",
    "df = pd.read_parquet(DATA_FILE)\n",
    "\n",
    "print(f\"Successfully loaded data with shape: {df.shape}\")\n",
    "print(df[['ID', 'Knee_Side', 'KL_Grade']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd049ef-770c-4b06-81f3-264653019a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Reload the Data ---\n",
    "print(\"--- 1. Reloading Data ---\")\n",
    "df = pd.read_parquet(DATA_FILE)\n",
    "print(f\"Reloaded data with shape: {df.shape}\\n\")\n",
    "\n",
    "# --- 2. Clean Known Date Columns ---\n",
    "print(\"--- 2. Cleaning Baseline, TKR, and Death Dates ---\")\n",
    "\n",
    "# Define key date columns\n",
    "baseline_date_col = 'V00EVDATE'   # Baseline visit\n",
    "right_tkr_col = 'V99ERKDATE'      # Right Knee TKR\n",
    "left_tkr_col = 'V99ELKDATE'       # Left Knee TKR\n",
    "death_date_col = 'V99EDDDATE'     # Date of Death\n",
    "\n",
    "# Define known \"missing\" strings\n",
    "missing_strings = ['.: Missing Form/Incomplete Workbook', '.: Missing'] \n",
    "\n",
    "# Pre-clean and convert with explicit formats\n",
    "for col, fmt in [\n",
    "    (baseline_date_col, '%m/%d/%Y'), # e.g., 06/03/2005\n",
    "    (right_tkr_col, '%m/%d/%y'),      # e.g., 04/04/06\n",
    "    (left_tkr_col, '%m/%d/%y'),       # e.g., 10/28/08\n",
    "    (death_date_col, '%m/%d/%Y')      # e.g., 11/15/2015\n",
    "]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].replace(missing_strings, np.nan)\n",
    "        df[col] = pd.to_datetime(df[col], format=fmt, errors='coerce')\n",
    "        print(f\"Column '{col}' NaT (Missing) Count: {df[col].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba03d6d1-538f-4a94-95c4-5a5f14cddbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. (Corrected) Build Master Visit-Date Lookup Table ---\n",
    "import glob\n",
    "import re\n",
    "\n",
    "print(\"--- 3. Building Visit-Date Lookup ---\")\n",
    "DATA_DIR = '../data/OAICompleteData_ASCII'\n",
    "all_clinical_files = glob.glob(f\"{DATA_DIR}/AllClinical*.txt\")\n",
    "\n",
    "visit_data_list = []\n",
    "\n",
    "# --- Step A: Handle Baseline (Visit 00) ---\n",
    "# We already have this date in our main 'df'\n",
    "df_baseline_dates = df[['ID', 'V00EVDATE']].copy()\n",
    "df_baseline_dates.rename(columns={'V00EVDATE': 'VISIT_DATE'}, inplace=True)\n",
    "df_baseline_dates['VISIT_NUM'] = '0' # Baseline is visit '0'\n",
    "visit_data_list.append(df_baseline_dates)\n",
    "\n",
    "# --- Step B: Loop through all Follow-up Visits (01, 02, ... 14) ---\n",
    "visit_regex = re.compile(r'AllClinical(\\d{2}).txt')\n",
    "\n",
    "for file_path in all_clinical_files:\n",
    "    match = visit_regex.search(file_path)\n",
    "    if not match:\n",
    "        continue\n",
    "    \n",
    "    visit_num_str = match.group(1) # e.g., '01', '08'\n",
    "    \n",
    "    # Skip baseline (00), we already did it\n",
    "    if visit_num_str == '00':\n",
    "        continue\n",
    "    \n",
    "    # This is the corrected column name pattern we discovered\n",
    "    date_col = f\"V{visit_num_str}FVDATE\" # e.g., V01FVDATE, V08FVDATE\n",
    "    \n",
    "    try:\n",
    "        # Load *only* the ID and the correct date column\n",
    "        df_visit = pd.read_csv(file_path, sep='|', on_bad_lines='skip', \n",
    "                             usecols=['ID', date_col])\n",
    "        \n",
    "        # Clean the date\n",
    "        df_visit[date_col] = pd.to_datetime(df_visit[date_col], format='%m/%d/%Y', errors='coerce')\n",
    "        \n",
    "        # Standardize columns\n",
    "        df_visit['VISIT_NUM'] = visit_num_str.lstrip('0') # '01' -> '1', '08' -> '8'\n",
    "        df_visit.rename(columns={date_col: 'VISIT_DATE'}, inplace=True)\n",
    "        \n",
    "        visit_data_list.append(df_visit[['ID', 'VISIT_NUM', 'VISIT_DATE']])\n",
    "        \n",
    "    except ValueError as e:\n",
    "        # This will catch any files that *still* don't match (e.g., don't have V...FVDATE)\n",
    "        print(f\"Skipping file {file_path}. Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"General error on {file_path}: {e}\")\n",
    "\n",
    "# --- Step C: Combine all visit data ---\n",
    "df_visit_lookup = pd.concat(visit_data_list, ignore_index=True)\n",
    "df_visit_lookup = df_visit_lookup.dropna(subset=['ID', 'VISIT_NUM', 'VISIT_DATE'])\n",
    "df_visit_lookup = df_visit_lookup.drop_duplicates(subset=['ID', 'VISIT_NUM'], keep='first')\n",
    "\n",
    "print(f\"\\nSUCCESS: Built visit lookup table with {df_visit_lookup.shape[0]} unique (ID, Visit) entries.\")\n",
    "print(df_visit_lookup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27eea9e-27f4-4421-812a-118fa998def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Finalize Survival Logic ---\n",
    "print(\"--- 4. Finalizing Survival Logic ---\")\n",
    "\n",
    "# 1. Clean V99RNTCNT (e.g., '14: Year 16' -> '14')\n",
    "df['LAST_VISIT_NUM'] = df['V99RNTCNT'].astype(str).str.split(':').str[0].str.strip()\n",
    "print(f\"Cleaned 'V99RNTCNT'. Example LAST_VISIT_NUMs: {df['LAST_VISIT_NUM'].unique()[:5]}\")\n",
    "\n",
    "# 2. Merge with our new lookup table to find the date of the last contact\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    df_visit_lookup,\n",
    "    left_on=['ID', 'LAST_VISIT_NUM'],\n",
    "    right_on=['ID', 'VISIT_NUM'],\n",
    "    how='left'\n",
    ")\n",
    "df.rename(columns={'VISIT_DATE': 'LAST_CONTACT_DATE'}, inplace=True)\n",
    "print(f\"Merged with lookup. Found {df['LAST_CONTACT_DATE'].notna().sum()} last contact dates.\")\n",
    "\n",
    "# 3. TKR Date (from Cell 2)\n",
    "df['TKR_Date'] = np.where(df['Knee_Side'] == 1, df[right_tkr_col], df[left_tkr_col])\n",
    "\n",
    "# 4. Event (from Cell 2)\n",
    "df['event'] = np.where(df['TKR_Date'].notna(), 1, 0)\n",
    "\n",
    "# 5. Censor Date is the *earliest* of (Last Contact Date) or (Date of Death)\n",
    "df['CENSOR_DATE'] = df[['LAST_CONTACT_DATE', death_date_col]].min(axis=1)\n",
    "\n",
    "# 6. End Date is TKR_Date (if event) or CENSOR_DATE (if no event)\n",
    "df['end_date'] = df['TKR_Date'].fillna(df['CENSOR_DATE'])\n",
    "\n",
    "# 7. Time to Event\n",
    "df['time_to_event'] = (df['end_date'] - df[baseline_date_col]).dt.days\n",
    "\n",
    "# --- 8. Inspect Final Results ---\n",
    "print(\"\\n--- 5. Final Survival Variable Inspection ---\")\n",
    "print(f\"Total TKR Events: {df['event'].sum()} (out of {df.shape[0]} knees)\")\n",
    "\n",
    "invalid_time = df[df['time_to_event'] < 0].shape[0]\n",
    "print(f\"Knees with invalid (< 0) time: {invalid_time}\")\n",
    "\n",
    "# Drop rows with no time information (no baseline or end date)\n",
    "original_rows = df.shape[0]\n",
    "df = df.dropna(subset=['time_to_event'])\n",
    "df = df[df['time_to_event'] >= 0] # Keep only valid time\n",
    "print(f\"Dropped {original_rows - df.shape[0]} rows with missing/invalid time.\")\n",
    "print(f\"Final usable cohort shape: {df.shape}\\n\")\n",
    "\n",
    "print(\"--- Sample of Censored Knees (event=0) ---\")\n",
    "print(df[df['event'] == 0][['ID', 'Knee_Side', 'time_to_event', 'event']].head())\n",
    "print(\"\\n--- Sample of TKR Knees (event=1) ---\")\n",
    "print(df[df['event'] == 1][['ID', 'Knee_Side', 'time_to_event', 'event']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4cd99-8c14-4dfd-9196-ef8ab5f113cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Find Baseline Clinical Features ---\n",
    "\n",
    "# List of keywords for features we want\n",
    "feature_keywords = [\n",
    "    'AGE',    # Patient Age\n",
    "    'SEX',    # Patient Sex\n",
    "    'BMI',    # Body Mass Index\n",
    "    'WOM',    # WOMAC Score (e.g., V00WOMTSR for Right Knee)\n",
    "]\n",
    "\n",
    "print(\"--- Searching for Clinical Features ---\")\n",
    "found_features = []\n",
    "for keyword in feature_keywords:\n",
    "    # Find columns that contain the keyword\n",
    "    matches = [col for col in df.columns if keyword in col.upper()]\n",
    "    if matches:\n",
    "        print(f\"Keyword '{keyword}' found in: {matches}\")\n",
    "        found_features.extend(matches)\n",
    "    else:\n",
    "        print(f\"Keyword '{keyword}' NOT found.\")\n",
    "\n",
    "# Add our key identifiers and targets\n",
    "key_cols = [\n",
    "    'ID', \n",
    "    'Knee_Side', \n",
    "    'KL_Grade',       # Our baseline image feature\n",
    "    'time_to_event', \n",
    "    'event'\n",
    "]\n",
    "final_feature_list = key_cols + list(set(found_features)) # Use set() to remove duplicates\n",
    "\n",
    "print(f\"\\n--- Final Selected Columns (Hypothesis) ---\")\n",
    "print(final_feature_list)\n",
    "\n",
    "# Let's inspect the values for these columns\n",
    "print(\"\\n--- Inspecting Values ---\")\n",
    "print(df[final_feature_list].head())\n",
    "\n",
    "# Check for missing values in this subset\n",
    "print(\"\\n--- Missing Value Counts ---\")\n",
    "print(df[final_feature_list].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c902e2-d482-4950-9cd5-5778a5ae97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Clean and Select Final Clinical/Survival Features ---\n",
    "\n",
    "# 1. Define a helper function to clean OAI's messy numeric columns\n",
    "#    (e.g., '1: Male' -> 1, '71.0' -> 71.0, '.: Missing...' -> NaN)\n",
    "def clean_numeric_col(series):\n",
    "    # First, replace all non-numeric \"missing\" strings with NaN\n",
    "    series = series.replace(\n",
    "        ['.: Missing Form/Incomplete Workbook', '.: Missing', '.'], np.nan\n",
    "    )\n",
    "    # Second, extract the numeric part (handles '1: Male' -> '1')\n",
    "    series_str = series.astype(str).str.split(':').str[0].str.strip()\n",
    "    # Convert to numeric, forcing errors (like 'nan') to NaN\n",
    "    return pd.to_numeric(series_str, errors='coerce')\n",
    "\n",
    "# --- 2. Define Our Final Feature Set ---\n",
    "# These are the *only* columns we need for our baseline model.\n",
    "key_cols = [\n",
    "    'ID', \n",
    "    'Knee_Side', \n",
    "    'time_to_event', \n",
    "    'event'\n",
    "]\n",
    "image_feature_cols = ['KL_Grade'] # Modality 1\n",
    "clinical_feature_cols = [\n",
    "    'V00AGE',    # Baseline Age\n",
    "    'P02SEX',    # Sex\n",
    "    'P01BMI',    # Baseline BMI\n",
    "    'V00WOMTSR', # Baseline WOMAC Total Score (Right)\n",
    "    'V00WOMTSL'  # Baseline WOMAC Total Score (Left)\n",
    "]\n",
    "\n",
    "# 3. Create our final model-ready DataFrame\n",
    "df_model_data = df[key_cols + image_feature_cols + clinical_feature_cols].copy()\n",
    "print(f\"Created df_model_data with shape: {df_model_data.shape}\\n\")\n",
    "\n",
    "# --- 4. Clean and Engineer Features ---\n",
    "print(\"Cleaning and engineering features...\")\n",
    "\n",
    "# Clean the simple numeric columns\n",
    "df_model_data['Age'] = clean_numeric_col(df_model_data['V00AGE'])\n",
    "df_model_data['BMI'] = clean_numeric_col(df_model_data['P01BMI'])\n",
    "\n",
    "# Clean Sex ('1: Male', '2: Female')\n",
    "df_model_data['Sex'] = clean_numeric_col(df_model_data['P02SEX'])\n",
    "\n",
    "# Clean the two WOMAC score columns\n",
    "womac_r_clean = clean_numeric_col(df_model_data['V00WOMTSR'])\n",
    "womac_l_clean = clean_numeric_col(df_model_data['V00WOMTSL'])\n",
    "\n",
    "# *** Key Engineering Step ***\n",
    "# Create a single 'WOMAC_Score' column that selects the correct\n",
    "# score based on the 'Knee_Side' for that row.\n",
    "df_model_data['WOMAC_Score'] = np.where(\n",
    "    df_model_data['Knee_Side'] == 1, # If Side is 1 (Right)\n",
    "    womac_r_clean,                  # Use the Right WOMAC score\n",
    "    womac_l_clean                   # Else (Side is 2), use the Left WOMAC score\n",
    ")\n",
    "\n",
    "# --- 5. Final Cleanup ---\n",
    "# Drop the raw columns we've now replaced\n",
    "df_model_data = df_model_data.drop(columns=[\n",
    "    'V00AGE', 'P02SEX', 'P01BMI', 'V00WOMTSR', 'V00WOMTSL'\n",
    "])\n",
    "\n",
    "print(\"\\n--- Final Model-Ready Data Info ---\")\n",
    "df_model_data.info()\n",
    "\n",
    "print(\"\\n--- Final Missing Value Counts ---\")\n",
    "# This tells us how many rows we'll need to drop or impute\n",
    "print(df_model_data.isna().sum())\n",
    "\n",
    "print(\"\\n--- Final Data Head ---\")\n",
    "print(df_model_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800bd57-9486-46af-b71c-9346a4e9c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Handle Missing Data and Save Final Model-Ready File ---\n",
    "\n",
    "print(f\"Original shape: {df_model_data.shape}\")\n",
    "\n",
    "# 1. Drop all rows with any missing values\n",
    "df_model_final = df_model_data.dropna()\n",
    "\n",
    "print(f\"Shape after dropping NaN rows: {df_model_final.shape}\")\n",
    "print(f\"Total rows dropped: {df_model_data.shape[0] - df_model_final.shape[0]}\\n\")\n",
    "\n",
    "# 2. Define the final output path\n",
    "FINAL_OUTPUT_FILE = '../data/processed/OAI_model_ready_data.parquet'\n",
    "\n",
    "# 3. Save the final file\n",
    "try:\n",
    "    df_model_final.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "    print(f\"SUCCESS: Final model-ready data saved to:\")\n",
    "    print(FINAL_OUTPUT_FILE)\n",
    "    \n",
    "    # Verify by reloading it\n",
    "    df_reloaded = pd.read_parquet(FINAL_OUTPUT_FILE)\n",
    "    print(f\"\\nVerification: Reloaded file with shape {df_reloaded.shape}\")\n",
    "    print(\"All missing values handled:\")\n",
    "    print(df_reloaded.isna().sum())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2876346-1843-45c4-a4e7-ba5f35dfa854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
