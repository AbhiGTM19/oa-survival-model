{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc46b74-bb34-4f66-9e0a-5f0396d0b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: diffusers[torch]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UNet2DModel, DDPMScheduler\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'diffusers'"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: Install & Setup ---\n",
    "!pip install diffusers[\"torch\"] transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup Paths & Device\n",
    "sys.path.append('/kaggle/working/oa-survival-model/src')\n",
    "from dataset import TriModalDataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Training on: {DEVICE}\")\n",
    "\n",
    "# Kaggle Paths\n",
    "PARQUET_PATH = '/kaggle/input/oai-preprocessed-data/OAI_model_ready_data.parquet'\n",
    "IMAGE_ROOT = '/kaggle/input/knee-osteoarthritis-dataset-with-severity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e39575-c811-4006-9745-237e34f0e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: Semantic Encoder Architecture ---\n",
    "import torchvision.models as models\n",
    "\n",
    "class SemanticEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super().__init__()\n",
    "        # Use ResNet18 backbone\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # Remove the final FC layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Add a projection to our desired latent dimension\n",
    "        self.projection = nn.Linear(512, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [Batch, 3, 224, 224]\n",
    "        x = self.features(x) # -> [Batch, 512, 1, 1]\n",
    "        x = x.view(x.size(0), -1) # -> [Batch, 512]\n",
    "        z = self.projection(x)    # -> [Batch, latent_dim]\n",
    "        return z\n",
    "\n",
    "print(\"Semantic Encoder defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf245361-cdad-4064-adb9-09afe44752c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 3: Initialize Diffusion Components ---\n",
    "\n",
    "# 1. The Noise Scheduler\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "# 2. The UNet (Generator)\n",
    "# We use a standard 2D UNet outputting 3 channels (RGB)\n",
    "# We condition it on the 'class_labels' which will actually be our Latent Vector z\n",
    "unet = UNet2DModel(\n",
    "    sample_size=64,  # We process at 64x64 for speed (or 128 if GPU allows)\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(128, 128, 256, 256),\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"\n",
    "    ),\n",
    "    # This enables conditioning on a vector!\n",
    "    class_embed_type=\"identity\" \n",
    ").to(DEVICE)\n",
    "\n",
    "# 3. The Encoder\n",
    "encoder = SemanticEncoder(latent_dim=128).to(DEVICE) # Match UNet embedding dim\n",
    "\n",
    "# 4. Optimizer (Train both)\n",
    "optimizer = torch.optim.Adam(list(unet.parameters()) + list(encoder.parameters()), lr=1e-4)\n",
    "\n",
    "print(\"Diffusion Model & Encoder initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a385f463-39fa-435b-aa46-ffe29780b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 4: Data Prep & Training ---\n",
    "\n",
    "# 1. Data Loader (Same as before, simplified transforms)\n",
    "# We resize to 64x64 for faster Diffusion training on T4\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]) # Diffusion models prefer -1 to 1 range\n",
    "])\n",
    "\n",
    "# Reuse your dataframe logic from previous notebooks to get 'df'\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "train_df, _ = train_test_split(df, test_size=0.1)\n",
    "\n",
    "dataset = TriModalDataset(train_df, IMAGE_ROOT, transform=train_transform, mode='sandbox')\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2. Training Loop\n",
    "EPOCHS = 5 # Keep it short for the prototype\n",
    "\n",
    "print(\"Starting Diffusion Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    unet.train()\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, _, _, _ in tqdm(loader):\n",
    "        images = images.to(DEVICE)\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # A. Encode Image to Semantic Latent z\n",
    "        # This vector 'z' captures \"Patient Anatomy\"\n",
    "        z = encoder(images) \n",
    "        \n",
    "        # B. Sample Noise\n",
    "        noise = torch.randn_like(images)\n",
    "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batch_size,), device=DEVICE).long()\n",
    "        \n",
    "        # C. Add Noise (Forward Diffusion)\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "        \n",
    "        # D. Predict Noise (Reverse Diffusion)\n",
    "        # Crucial: We pass 'z' as class_labels to condition the generation\n",
    "        noise_pred = unet(noisy_images, timestep=timesteps, class_labels=z).sample\n",
    "        \n",
    "        # E. Loss (MSE between real noise and predicted noise)\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606f357-36c2-439a-8f7a-0eff5080f051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 5: Generate Counterfactual ---\n",
    "\n",
    "def generate_counterfactual(original_image, modification_factor=0.0):\n",
    "    # 1. Encode Real Image -> Latent z\n",
    "    with torch.no_grad():\n",
    "        original_image = original_image.unsqueeze(0).to(DEVICE) # Batch of 1\n",
    "        z = encoder(original_image)\n",
    "        \n",
    "        # 2. MODIFY Latent z (The Counterfactual Step)\n",
    "        # In a real scenario, we would use gradients from the survival model.\n",
    "        # Here, we simulate a change by adding random noise for demonstration.\n",
    "        z_modified = z + (torch.randn_like(z) * modification_factor)\n",
    "\n",
    "        # 3. Generate Image from Modified z\n",
    "        # Start from pure noise\n",
    "        generated_image = torch.randn_like(original_image)\n",
    "        \n",
    "        # Denoise loop\n",
    "        for t in scheduler.timesteps:\n",
    "            # Expand z for the batch\n",
    "            model_output = unet(generated_image, t, class_labels=z_modified).sample\n",
    "            generated_image = scheduler.step(model_output, t, generated_image).prev_sample\n",
    "\n",
    "    return generated_image.cpu().squeeze()\n",
    "\n",
    "# Test it\n",
    "# Get a sample image\n",
    "sample_img, _, _, _ = dataset[0] \n",
    "# Generate: 0.0 modification = Reconstruction, 0.5 = Counterfactual\n",
    "recon_img = generate_counterfactual(sample_img, modification_factor=0.0)\n",
    "cf_img = generate_counterfactual(sample_img, modification_factor=1.0)\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(sample_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "axs[0].set_title(\"Original Patient X-Ray\")\n",
    "axs[1].imshow(recon_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "axs[1].set_title(\"AI Reconstruction (Sanity Check)\")\n",
    "axs[2].imshow(cf_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "axs[2].set_title(\"Survival Counterfactual (Modified)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
