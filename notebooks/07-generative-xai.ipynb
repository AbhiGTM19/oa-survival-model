{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc46b74-bb34-4f66-9e0a-5f0396d0b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 'src' module loaded.\n",
      "Training on: Apple M1 GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: Local Setup (Mac M1) ---\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Diffusers imports\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# 1. Setup Paths\n",
    "# Add '../src' to the python path so we can import our modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# 2. Verify Imports\n",
    "try:\n",
    "    from dataset import TriModalDataset\n",
    "    print(\"Success: 'src' module loaded.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: Could not import from src. {e}\")\n",
    "\n",
    "# 3. Setup Device (Use Metal Performance Shaders for M1)\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Training on: Apple M1 GPU (MPS)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Training on: CPU (Warning: Slow)\")\n",
    "\n",
    "# 4. Define Local Data Paths\n",
    "PARQUET_PATH = '../data/processed/OAI_model_ready_data.parquet'\n",
    "IMAGE_ROOT = '../data/sandbox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8e39575-c811-4006-9745-237e34f0e1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI Architecture Updated (Grayscale Enabled).\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 2: Generative AI Setup (Grayscale) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Redefine Encoder for 1-Channel Input\n",
    "class SemanticEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=256): \n",
    "        super().__init__()\n",
    "        # ResNet expects 3 channels. We modify the first layer to take 1 channel.\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # The magic fix: Change input layer from 3 channels to 1\n",
    "        original_first_layer = resnet.conv1\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Initialize the new 1-channel weights by averaging the old RGB weights\n",
    "        with torch.no_grad():\n",
    "            resnet.conv1.weight[:] = original_first_layer.weight.sum(dim=1, keepdim=True) / 3.0\n",
    "            \n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.projection = nn.Linear(512, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.projection(x)\n",
    "        return z\n",
    "\n",
    "print(\"Generative AI Architecture Updated (Grayscale Enabled).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee7459-81ab-47f3-8db7-9e299f18661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grayscale Dataset: Found 9786 images.\n",
      "Starting Grayscale Training (50 Epochs)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b47a8e4e04843639118f131af73c4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL 3: Train Optimized Diffusion Model ---\n",
    "\n",
    "# 1. Configuration\n",
    "GEN_EPOCHS = 50  # Increased for better quality\n",
    "LR = 1e-4\n",
    "\n",
    "# 2. Initialize Models (1-Channel Configuration)\n",
    "unet = UNet2DModel(\n",
    "    sample_size=64,  \n",
    "    in_channels=1,   # CHANGED: Grayscale\n",
    "    out_channels=1,  # CHANGED: Grayscale\n",
    "    layers_per_block=2,\n",
    "    block_out_channels=(64, 128, 128, 256),\n",
    "    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n",
    "    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n",
    "    class_embed_type=\"identity\"\n",
    ").to(DEVICE)\n",
    "\n",
    "encoder = SemanticEncoder(latent_dim=256).to(DEVICE)\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = torch.optim.Adam(list(unet.parameters()) + list(encoder.parameters()), lr=LR)\n",
    "\n",
    "# 3. Transforms (1-Channel)\n",
    "gen_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]) # Mean/Std for 1 channel\n",
    "])\n",
    "\n",
    "class GrayscaleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        # Get all images using glob (recursively)\n",
    "        import glob\n",
    "        self.all_image_paths = glob.glob(f\"{image_dir}/**/*.png\", recursive=True) + \\\n",
    "                               glob.glob(f\"{image_dir}/**/*.jpg\", recursive=True)\n",
    "        print(f\"Grayscale Dataset: Found {len(self.all_image_paths)} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Sandbox Mode Logic: Pick Random Image\n",
    "        img_path = np.random.choice(self.all_image_paths)\n",
    "        \n",
    "        # Force Grayscale ('L')\n",
    "        from PIL import Image\n",
    "        image = Image.open(img_path).convert('L') \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Load Data (Local Paths)\n",
    "df = pd.read_parquet('../data/processed/OAI_model_ready_data.parquet')\n",
    "train_df, _ = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize Dataset & Loader\n",
    "gen_dataset = GrayscaleDataset(train_df.head(200), '../data/sandbox', transform=gen_transform)\n",
    "gen_loader = DataLoader(gen_dataset, batch_size=32, shuffle=True) \n",
    "# Note: We use a small subset (head(200)) for local speed test\n",
    "\n",
    "# 5. Training Loop\n",
    "print(f\"Starting Grayscale Training ({GEN_EPOCHS} Epochs)...\")\n",
    "\n",
    "for epoch in range(GEN_EPOCHS):\n",
    "    unet.train()\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    progress = tqdm(gen_loader, desc=f\"Epoch {epoch+1}/{GEN_EPOCHS}\", leave=False)\n",
    "    \n",
    "    for images in progress:\n",
    "        images = images.to(DEVICE) # Shape: [Batch, 1, 64, 64]\n",
    "        batch_size = images.shape[0]\n",
    "        \n",
    "        # Encode\n",
    "        z = encoder(images)\n",
    "        \n",
    "        # Add Noise\n",
    "        noise = torch.randn_like(images)\n",
    "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batch_size,), device=DEVICE).long()\n",
    "        noisy_images = scheduler.add_noise(images, noise, timesteps)\n",
    "        \n",
    "        # Predict Noise\n",
    "        noise_pred = unet(noisy_images, timestep=timesteps, class_labels=z).sample\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.mse_loss(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} | Avg Loss: {total_loss/len(gen_loader):.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40b066-c6d0-4bf5-be52-4e70b2766197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 4: Visualization (Grayscale) ---\n",
    "\n",
    "def generate_image(ref_image, modification=0.0):\n",
    "    unet.eval()\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        ref_image = ref_image.unsqueeze(0).to(DEVICE)\n",
    "        z = encoder(ref_image)\n",
    "        \n",
    "        # Simulate Counterfactual\n",
    "        z_modified = z + (torch.randn_like(z) * modification)\n",
    "        \n",
    "        # Generate\n",
    "        image = torch.randn_like(ref_image)\n",
    "        for t in scheduler.timesteps:\n",
    "            out = unet(image, t, class_labels=z_modified).sample\n",
    "            image = scheduler.step(out, t, image).prev_sample\n",
    "            \n",
    "    return image.cpu().squeeze()\n",
    "\n",
    "# Pick a sample\n",
    "sample = gen_dataset[0]\n",
    "recon = generate_image(sample, modification=0.0)\n",
    "counterfactual = generate_image(sample, modification=1.0)\n",
    "\n",
    "# Display in Grayscale\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].imshow(sample.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "axs[0].set_title(\"Original X-Ray\")\n",
    "axs[1].imshow(recon.squeeze(), cmap='gray')\n",
    "axs[1].set_title(\"AI Reconstruction\")\n",
    "axs[2].imshow(counterfactual.squeeze(), cmap='gray')\n",
    "axs[2].set_title(\"Counterfactual\")\n",
    "plt.show()\n",
    "\n",
    "# Save models\n",
    "torch.save(unet.state_dict(), \"diffusion_unet.pth\")\n",
    "torch.save(encoder.state_dict(), \"semantic_encoder.pth\")\n",
    "print(\"Grayscale models saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
