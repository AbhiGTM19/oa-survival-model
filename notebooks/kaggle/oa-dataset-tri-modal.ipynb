{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2097406,"sourceType":"datasetVersion","datasetId":1257880},{"sourceId":13955145,"sourceType":"datasetVersion","datasetId":8893735}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- CELL 1: Setup, Sync & Install ---\nimport sys\nimport os\n\n# 1. Clean up old repo\n!rm -rf oa-survival-model\n\n# 2. Clone your repo\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngit_token = user_secrets.get_secret(\"GIT_TOKEN\")\nusername = \"AbhiGTM19\"\n\n!git clone https://{username}:{git_token}@github.com/{username}/oa-survival-model.git\n\n# 3. Install ALL dependencies from your updated requirements.txt\n# This will now include 'diffusers' and 'transformers' automatically\n%cd oa-survival-model\n!pip install -r requirements.txt\n%cd ..\n\n# 4. Add source code to path\nsys.path.append('/kaggle/working/oa-survival-model/src')\n\nprint(\"Environment Ready & Dependencies Installed.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 2: Imports & Configuration (Multi-GPU Optimized) ---\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nimport torchsurv.loss\nfrom torch.cuda.amp import GradScaler, autocast # For Mixed Precision\n\n# Import your custom modules\nfrom model import WideAndDeepSurvivalModel\nfrom dataset import TriModalDataset\n\n# Configuration\nif torch.cuda.is_available():\n    DEVICE = torch.device(\"cuda\")\n    GPU_COUNT = torch.cuda.device_count()\n    print(f\"Training on: {GPU_COUNT} x NVIDIA GPU(s)\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n    GPU_COUNT = 0\n    print(\"Training on: CPU\")\n\n# Paths\nPARQUET_PATH = '/kaggle/input/oai-preprocessed-data/OAI_tri_modal_real.parquet' \nIMAGE_ROOT = '/kaggle/input/knee-osteoarthritis-dataset-with-severity'\n\n# Hyperparameters\n# Scale batch size by number of GPUs (32 per GPU -> 64 total)\nBATCH_SIZE = 32 * max(1, GPU_COUNT) \nEPOCHS = 10\nLEARNING_RATE = 1e-4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 3: Data Prep (Exact copy of your local logic) ---\n\n# 1. Load Data\ndf = pd.read_parquet(PARQUET_PATH)\n\n# 2. Preprocessing\n# We recreate the columns manually for simplicity in this test script\ndf = pd.get_dummies(df, columns=['KL_Grade', 'Sex'], drop_first=True)\nexpected_cols = ['KL_Grade_1.0', 'KL_Grade_2.0', 'KL_Grade_3.0', 'KL_Grade_4.0', 'Sex_2']\nfor col in expected_cols:\n    if col not in df.columns:\n        df[col] = 0\n\n# 3. Split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# 4. Transforms\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(), # Augmentation\n    transforms.RandomRotation(10),     # Augmentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# 5. Datasets & Loaders\ntrain_dataset = TriModalDataset(train_df, IMAGE_ROOT, transform=train_transform, mode='sandbox')\nval_dataset = TriModalDataset(val_df, IMAGE_ROOT, transform=val_transform, mode='sandbox')\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nprint(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 4: Tri-Modal Survival Training ---\n\n# 1. Initialize Model (With Biomarker Input)\n# Note: bio_input_dim=5 matches your 5 chosen markers\nmodel = WideAndDeepSurvivalModel(wide_input_dim=8, bio_input_dim=5).to(DEVICE)\n\nif GPU_COUNT > 1:\n    model = nn.DataParallel(model)\n\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscaler = GradScaler()\n\n# ... (Loss function definition stays the same) ...\n\n# 3. Training Loop\nprint(\"Starting Tri-Modal Training...\")\nfor epoch in range(EPOCHS):\n    model.train()\n    total_loss = 0\n    \n    # UPDATE UNPACKING: Now getting 5 items\n    for batch_idx, (images, clinical, bio, events, times) in enumerate(train_loader):\n        if events.sum() == 0: continue\n\n        # Move ALL 3 inputs to GPU\n        images = images.to(DEVICE)\n        clinical = clinical.to(DEVICE)\n        bio = bio.to(DEVICE) # <--- New Input\n        events = events.to(DEVICE)\n        times = times.to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        with autocast():\n            # Pass ALL 3 inputs to model\n            risk_scores = model(images, clinical, bio).squeeze()\n            loss = cox_loss_func(risk_scores, events, times)\n        \n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        \n    print(f\"Epoch {epoch+1}/{EPOCHS} | Avg Loss: {total_loss/len(train_loader):.4f}\")\n\n# Save the Tri-Modal Model\ntorch.save(model.module.state_dict() if GPU_COUNT > 1 else model.state_dict(), \"tri_modal_survival_model.pth\")\nprint(\"Saved Tri-Modal Model.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 5: Evaluation ---\nfrom sksurv.metrics import concordance_index_censored\nimport numpy as np\n\nprint(\"Evaluating on Validation Set...\")\nmodel.eval()\n\nval_risk_scores = []\nval_events = []\nval_times = []\n\n# 1. Collect predictions\nwith torch.no_grad():\n    for images, clinical, events, times in val_loader:\n        images, clinical = images.to(DEVICE), clinical.to(DEVICE)\n        \n        # Forward pass\n        outputs = model(images, clinical).squeeze()\n        \n        # Store results (move to CPU for sksurv)\n        val_risk_scores.extend(outputs.cpu().numpy())\n        val_events.extend(events.numpy().astype(bool)) # sksurv expects boolean\n        val_times.extend(times.numpy())\n\n# 2. Calculate C-index\nc_index = concordance_index_censored(\n    np.array(val_events),\n    np.array(val_times),\n    np.array(val_risk_scores)\n)\n\nprint(f\"Validation C-index: {c_index[0]:.4f}\")\nprint(f\"Baseline Target:    0.7468\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 6: Save Model ---\nSAVE_PATH = \"tri_modal_survival_model.pth\"\ntorch.save(model.state_dict(), SAVE_PATH)\nprint(f\"Model saved to: {SAVE_PATH}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 7: Improved Generative AI Setup (Grayscale) ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom diffusers import UNet2DModel, DDPMScheduler\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport torchvision.models as models\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport numpy as np\nimport os\nimport glob\n\n# Redefine Encoder for 1-Channel Input\nclass SemanticEncoder(nn.Module):\n    def __init__(self, latent_dim=256): \n        super().__init__()\n        # ResNet expects 3 channels. We modify the first layer to take 1 channel.\n        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n        \n        # The magic fix: Change input layer from 3 channels to 1\n        original_first_layer = resnet.conv1\n        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        # Initialize the new 1-channel weights by averaging the old RGB weights\n        with torch.no_grad():\n            resnet.conv1.weight[:] = original_first_layer.weight.sum(dim=1, keepdim=True) / 3.0\n            \n        self.features = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection = nn.Linear(512, latent_dim)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        z = self.projection(x)\n        return z\n\nprint(\"Generative AI Architecture Updated (Grayscale Enabled).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 8: Generative AI Training (Robust Self-Contained Version) ---\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom diffusers import UNet2DModel, DDPMScheduler\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport torchvision.models as models\nfrom PIL import Image\nimport numpy as np\nimport os\nimport glob\nfrom torch.cuda.amp import GradScaler, autocast\n\n# --- 1. CONFIGURATION ---\nGEN_EPOCHS = 500   # The \"Magic Number\" for high quality\nLR = 1e-4\nBATCH_SIZE_PER_GPU = 32\n\n# Check Device\nif torch.cuda.is_available():\n    DEVICE = torch.device(\"cuda\")\n    GPU_COUNT = torch.cuda.device_count()\n    print(f\"Training on: {GPU_COUNT} x NVIDIA GPU(s)\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n    GPU_COUNT = 1\n    print(\"Training on: CPU\")\n\n# Create Checkpoint Folder\nif not os.path.exists(\"checkpoints\"):\n    os.makedirs(\"checkpoints\")\n\n# --- 2. ROBUST CLASS DEFINITIONS ---\n# We define these HERE to ensure the Grayscale logic is 100% active\n# and to avoid import errors if model.py isn't synced.\n\nclass SemanticEncoder(nn.Module):\n    def __init__(self, latent_dim=256): \n        super().__init__()\n        # Load ResNet\n        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n        \n        # --- GRAYSCALE FIX (3 Channels -> 1 Channel) ---\n        original_first_layer = resnet.conv1\n        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        with torch.no_grad():\n            # Average the weights to keep pre-trained patterns\n            resnet.conv1.weight[:] = original_first_layer.weight.sum(dim=1, keepdim=True) / 3.0\n            \n        self.features = nn.Sequential(*list(resnet.children())[:-1])\n        self.projection = nn.Linear(512, latent_dim)\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        z = self.projection(x)\n        return z\n\nclass GrayscaleDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transform=None):\n        self.df = dataframe\n        self.transform = transform\n        # Find all images recursively\n        self.all_image_paths = glob.glob(f\"{image_dir}/**/*.png\", recursive=True) + \\\n                               glob.glob(f\"{image_dir}/**/*.jpg\", recursive=True)\n        print(f\"Grayscale Dataset: Found {len(self.all_image_paths)} images.\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Robust Training Strategy: Random Sampling\n        # This ensures the model sees ALL variations, not just the limited rows in df\n        img_path = np.random.choice(self.all_image_paths)\n        \n        try:\n            # Force Convert to Grayscale ('L')\n            image = Image.open(img_path).convert('L') \n            if self.transform:\n                image = self.transform(image)\n            return image\n        except Exception as e:\n            # Fallback for bad images\n            return torch.zeros(1, 64, 64)\n\n# --- 3. INITIALIZE MODELS ---\nprint(\"Initializing Grayscale Models...\")\nunet = UNet2DModel(\n    sample_size=64,  \n    in_channels=1,   # Grayscale Input\n    out_channels=1,  # Grayscale Output\n    layers_per_block=2,\n    block_out_channels=(64, 128, 128, 256),\n    down_block_types=(\"DownBlock2D\", \"DownBlock2D\", \"AttnDownBlock2D\", \"DownBlock2D\"),\n    up_block_types=(\"UpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\", \"UpBlock2D\"),\n    class_embed_type=\"identity\" # Allows conditioning on 'z'\n).to(DEVICE)\n\nencoder = SemanticEncoder(latent_dim=256).to(DEVICE)\n\n# Multi-GPU Wrapping\nif GPU_COUNT > 1:\n    print(\">> Activating DataParallel for UNet & Encoder\")\n    unet = nn.DataParallel(unet)\n    encoder = nn.DataParallel(encoder)\n\n# Optimizer & Scheduler\nscheduler = DDPMScheduler(num_train_timesteps=1000)\noptimizer = torch.optim.Adam(list(unet.parameters()) + list(encoder.parameters()), lr=LR)\nscaler = GradScaler() # Mixed Precision for Speed\n\n# --- 4. DATA LOADING ---\ngen_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5]) # Normalize to [-1, 1]\n])\n\n# Use the paths defined in Cell 2\ngen_dataset = GrayscaleDataset(train_df, IMAGE_ROOT, transform=gen_transform)\ngen_loader = DataLoader(\n    gen_dataset, \n    batch_size=BATCH_SIZE_PER_GPU * max(1, GPU_COUNT), # Scale batch size\n    shuffle=True, \n    num_workers=2\n)\n\n# --- 5. TRAINING LOOP ---\nprint(f\"Starting Training: {GEN_EPOCHS} Epochs | Batch Size: {BATCH_SIZE_PER_GPU * max(1, GPU_COUNT)}\")\n\nfor epoch in range(GEN_EPOCHS):\n    unet.train()\n    encoder.train()\n    total_loss = 0\n    \n    # Progress bar for this epoch\n    progress = tqdm(gen_loader, desc=f\"Epoch {epoch+1}/{GEN_EPOCHS}\", leave=False)\n    \n    for images in progress:\n        images = images.to(DEVICE)\n        batch_size = images.shape[0]\n        \n        optimizer.zero_grad()\n        \n        # Mixed Precision Context (Faster on T4)\n        with autocast():\n            # 1. Encode Image -> Vector z\n            z = encoder(images)\n            \n            # 2. Add Noise to Image\n            noise = torch.randn_like(images)\n            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batch_size,), device=DEVICE).long()\n            noisy_images = scheduler.add_noise(images, noise, timesteps)\n            \n            # 3. Predict Noise (Conditioned on z)\n            # return_dict=False fixes the DataParallel tuple issue\n            noise_pred = unet(noisy_images, timestep=timesteps, class_labels=z, return_dict=False)[0]\n            \n            # 4. Calculate Loss\n            loss = F.mse_loss(noise_pred, noise)\n        \n        # Backward & Step (Scaled)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        \n        total_loss += loss.item()\n        progress.set_postfix({\"loss\": loss.item()})\n        \n    avg_loss = total_loss / len(gen_loader)\n    print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n\n    # --- SAVE CHECKPOINT (Every 50 Epochs) ---\n    if (epoch + 1) % 50 == 0:\n        # Handle unwrapping DataParallel for saving\n        unet_state = unet.module.state_dict() if isinstance(unet, nn.DataParallel) else unet.state_dict()\n        enc_state = encoder.module.state_dict() if isinstance(encoder, nn.DataParallel) else encoder.state_dict()\n        \n        torch.save(unet_state, f\"checkpoints/unet_epoch_{epoch+1}.pth\")\n        torch.save(enc_state, f\"checkpoints/encoder_epoch_{epoch+1}.pth\")\n        print(f\"--> Checkpoint saved at Epoch {epoch+1}\")\n\n# Final Save\nunet_state = unet.module.state_dict() if isinstance(unet, nn.DataParallel) else unet.state_dict()\nenc_state = encoder.module.state_dict() if isinstance(encoder, nn.DataParallel) else encoder.state_dict()\ntorch.save(unet_state, \"diffusion_unet.pth\")\ntorch.save(enc_state, \"semantic_encoder.pth\")\nprint(\"Generative Training Finished Successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- CELL 9: Visualization (Grayscale) ---\n\ndef generate_image(ref_image, modification=0.0):\n    unet.eval()\n    encoder.eval()\n    with torch.no_grad():\n        ref_image = ref_image.unsqueeze(0).to(DEVICE)\n        z = encoder(ref_image)\n        \n        # Simulate Counterfactual\n        z_modified = z + (torch.randn_like(z) * modification)\n        \n        # Generate\n        image = torch.randn_like(ref_image)\n        for t in scheduler.timesteps:\n            out = unet(image, t, class_labels=z_modified).sample\n            image = scheduler.step(out, t, image).prev_sample\n            \n    return image.cpu().squeeze()\n\n# Pick a sample\nsample = gen_dataset[0]\nrecon = generate_image(sample, modification=0.0)\ncounterfactual = generate_image(sample, modification=1.0)\n\n# Display in Grayscale\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\naxs[0].imshow(sample.permute(1, 2, 0).squeeze(), cmap='gray')\naxs[0].set_title(\"Original X-Ray\")\naxs[1].imshow(recon.squeeze(), cmap='gray')\naxs[1].set_title(\"AI Reconstruction\")\naxs[2].imshow(counterfactual.squeeze(), cmap='gray')\naxs[2].set_title(\"Counterfactual\")\nplt.show()\n\n# Save models\ntorch.save(unet.state_dict(), \"diffusion_unet.pth\")\ntorch.save(encoder.state_dict(), \"semantic_encoder.pth\")\nprint(\"Grayscale models saved.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}