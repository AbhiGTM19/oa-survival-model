{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbeabaf-4e94-458e-992b-9e07f83374e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "\n",
    "# 1. Load the final, model-ready dataset\n",
    "DATA_FILE = 'data/processed/OAI_model_ready_data.parquet'\n",
    "df = pd.read_parquet(DATA_FILE)\n",
    "\n",
    "print(f\"Loaded model-ready data with shape: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Define Features (X) and Target (y)\n",
    "# These are the 5 features for our baseline model\n",
    "FEATURE_COLS = ['KL_Grade', 'Age', 'BMI', 'Sex', 'WOMAC_Score']\n",
    "X = df[FEATURE_COLS]\n",
    "\n",
    "# These are our two survival target columns\n",
    "EVENT_COL = 'event'\n",
    "TIME_COL = 'time_to_event'\n",
    "\n",
    "# 3. Create the 'y' structured array for scikit-survival\n",
    "# This format (bool, float) is required by the library.\n",
    "y = np.empty(len(df), dtype=[\n",
    "    ('event', bool), \n",
    "    ('time_to_event', np.float64)\n",
    "])\n",
    "\n",
    "y['event'] = df[EVENT_COL].astype(bool)\n",
    "y['time_to_event'] = df[TIME_COL].astype(np.float64)\n",
    "\n",
    "print(f\"\\nCreated feature matrix X with shape: {X.shape}\")\n",
    "print(f\"Created target array y with shape: {y.shape}\")\n",
    "\n",
    "# 4. Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Split data into {len(X_train)} train and {len(X_test)} test samples.\")\n",
    "\n",
    "# --- 5. Preprocess the features (CORRECTED) ---\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define which columns are which\n",
    "categorical_cols = ['KL_Grade', 'Sex']\n",
    "numerical_cols = ['Age', 'BMI', 'WOMAC_Score']\n",
    "\n",
    "# Create a preprocessor\n",
    "# 1. OneHotEncoder: Expands categorical features. 'drop=first' avoids dummy variable trap.\n",
    "# 2. StandardScaler: Scales numerical features.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough' # Pass through any other columns (though we have none)\n",
    ")\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names for clarity\n",
    "feature_names = (\n",
    "    numerical_cols + \n",
    "    preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols).tolist()\n",
    ")\n",
    "\n",
    "print(\"Features successfully preprocessed (scaled and one-hot encoded).\")\n",
    "print(f\"Final processed training shape: {X_train_processed.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf63fa4-87a6-4032-bf82-cd7c8bffd4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# 1. Initialize the CoxPH model\n",
    "# We set a baseline alpha for regularization\n",
    "cox_model = CoxPHSurvivalAnalysis(alpha=0.1)\n",
    "\n",
    "# 2. Train the model\n",
    "# We use the processed features and the structured survival target\n",
    "cox_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# 3. Evaluate on the Test Set\n",
    "# Get the predicted risk scores for the test set\n",
    "test_risk_scores = cox_model.predict(X_test_processed)\n",
    "\n",
    "# Calculate the Concordance Index (C-index)\n",
    "# This measures how well the model's risk score predictions\n",
    "# match the actual order of events.\n",
    "# 1.0 is perfect, 0.5 is random chance.\n",
    "c_index_result = concordance_index_censored(\n",
    "    y_test['event'],      # Ground truth (did an event happen?)\n",
    "    y_test['time_to_event'], # Ground truth (how long did it take?)\n",
    "    test_risk_scores      # Our model's prediction\n",
    ")\n",
    "\n",
    "print(f\"--- Baseline Model Evaluation ---\")\n",
    "print(f\"Concordance Index (C-index) on Test Set: {c_index_result[0]:.4f}\")\n",
    "\n",
    "# 4. Show Feature Importance\n",
    "# Let's see what the model thinks is important\n",
    "feature_importance = pd.Series(\n",
    "    cox_model.coef_,\n",
    "    index=feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance (Model Coefficients) ---\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f6dfb-acfe-4f3b-88dd-e5294bb405a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
