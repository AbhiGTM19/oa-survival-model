{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c0d73b-a42f-4fa4-b007-08ff0a580157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Training Tri-Modal DenseNet on mps\n",
      "   Data Loaded. Shape: (3526, 28)\n",
      "   Starting Training Loop (177 batches per epoch)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da740da019a84f0c929626e659e5cfe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss 1.5369\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e7f2e13a2f4533ad125bccf18f262e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Average Loss 1.5605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b0b831e5aa4428836baad5de5367ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Average Loss 1.4963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5424b0702d6145c28bbc200badb3ce57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Average Loss 1.5181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5687c19587d54e12a237fe69a2b1527d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Average Loss 1.4780\n",
      "âœ… Saved DenseNet Tri-Modal model to ../models/tri_modal_survival_model.pth\n"
     ]
    }
   ],
   "source": [
    "# --- Local Tri-Modal Training Script (DenseNet + Platinum Data) ---\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from dataset import TriModalDataset\n",
    "from model import WideAndDeepSurvivalModel\n",
    "import torchsurv.loss\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# Use the Platinum Cohort (High Quality)\n",
    "PARQUET_PATH = '../data/processed/OAI_mega_cohort_imputed.parquet'\n",
    "IMAGE_ROOT = '../data/sandbox'\n",
    "\n",
    "# Platinum Cohort Dimensions: \n",
    "# 8 Basic (Age, BMI, WOMAC, Sex, KL*4) + 3 Advanced (KOOS, PASE, BML) = 11\n",
    "CLINICAL_INPUT_DIM = 11 \n",
    "BIOMARKER_INPUT_DIM = 5\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "\n",
    "print(f\"ðŸš€ Training Tri-Modal DenseNet on {DEVICE}\")\n",
    "\n",
    "# 1. Load Data\n",
    "if not os.path.exists(PARQUET_PATH):\n",
    "    raise FileNotFoundError(f\"Platinum cohort not found at {PARQUET_PATH}. Please run src/build_platinum_cohort.py first.\")\n",
    "\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "\n",
    "# One-Hot Encode Categorical Features\n",
    "# We need to ensure all columns expected by dataset.py exist\n",
    "df = pd.get_dummies(df, columns=['KL_Grade', 'Sex'], drop_first=True)\n",
    "\n",
    "# Force-create missing columns with 0s (Robustness)\n",
    "expected_cols = ['KL_Grade_1.0', 'KL_Grade_2.0', 'KL_Grade_3.0', 'KL_Grade_4.0', 'Sex_2']\n",
    "for col in expected_cols:\n",
    "    if col not in df.columns: \n",
    "        df[col] = 0\n",
    "\n",
    "print(f\"   Data Loaded. Shape: {df.shape}\")\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Transforms (DenseNet expects 224x224)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 3. Dataset\n",
    "train_dataset = TriModalDataset(train_df, IMAGE_ROOT, transform=train_transform, mode='sandbox')\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 4. Model Initialization\n",
    "# UPDATED: wide_input_dim=11 to match Platinum features\n",
    "model = WideAndDeepSurvivalModel(wide_input_dim=CLINICAL_INPUT_DIM, bio_input_dim=BIOMARKER_INPUT_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Loss function check (MPS Safe)\n",
    "try:\n",
    "    base_cox = torchsurv.loss.cox\n",
    "    def cox_loss(risk, events, times):\n",
    "        # MPS Fix: Move to CPU for logcumsumexp calculation\n",
    "        if risk.device.type == 'mps':\n",
    "            return base_cox(risk.cpu(), events.cpu(), times.cpu()).to(DEVICE)\n",
    "        return base_cox(risk, events, times)\n",
    "except:\n",
    "    # Custom fallback if library missing\n",
    "    def cox_loss(risk, events, times):\n",
    "        order = torch.argsort(times, descending=True)\n",
    "        risk = risk[order]\n",
    "        events = events[order]\n",
    "        \n",
    "        # CPU Fallback for cumulative sum\n",
    "        risk_cpu = risk.cpu()\n",
    "        log_cumsum = torch.logcumsumexp(risk_cpu, dim=0).to(risk.device)\n",
    "        \n",
    "        if events.sum() > 0:\n",
    "            return -torch.sum(events * (risk - log_cumsum)) / events.sum()\n",
    "        return torch.tensor(0.0, requires_grad=True).to(DEVICE)\n",
    "\n",
    "# 5. Train Loop\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"   Starting Training Loop ({len(train_loader)} batches per epoch)...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "    for batch_idx, batch in enumerate(progress):\n",
    "        # Unpack 5 items\n",
    "        img, clin, bio, event, time = batch\n",
    "        \n",
    "        img = img.to(DEVICE)\n",
    "        clin = clin.to(DEVICE)\n",
    "        bio = bio.to(DEVICE)\n",
    "        event = event.to(DEVICE)\n",
    "        time = time.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        risk = model(img, clin, bio).squeeze()\n",
    "        \n",
    "        # Loss\n",
    "        loss = cox_loss(risk, event, time)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        progress.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}: Average Loss {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# 6. Save\n",
    "SAVE_PATH = \"../models/tri_modal_survival_model.pth\"\n",
    "if not os.path.exists('../models'): os.makedirs('../models')\n",
    "torch.save(model.state_dict(), SAVE_PATH)\n",
    "print(f\"âœ… Saved DenseNet Tri-Modal model to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177233e-353b-4e2b-b19d-ace1719cc50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
