{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703ffbe-fb68-48b5-8cb6-2d058cdbbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the base path to our data\n",
    "DATA_DIR = '../data/OAICompleteData_ASCII'\n",
    "\n",
    "# --- 1. Define File Paths ---\n",
    "\n",
    "# Our Primary Key / Demographics\n",
    "enrollees_path = f\"{DATA_DIR}/Enrollees.txt\"\n",
    "\n",
    "# Our Survival Target (TKR)\n",
    "outcomes_path = f\"{DATA_DIR}/OUTCOMES99.txt\"\n",
    "\n",
    "# Modality 1: Image Labels (Baseline)\n",
    "xray_labels_path = f\"{DATA_DIR}/KXR_SQ_BU00.txt\"\n",
    "\n",
    "# Modality 2: Clinical Features (Baseline)\n",
    "clinical_path = f\"{DATA_DIR}/AllClinical00.txt\"\n",
    "\n",
    "# Modality 3: Genetic Features (Baseline)\n",
    "biomarkers_path = f\"{DATA_DIR}/Biomarkers00.txt\"\n",
    "\n",
    "print(f\"File paths defined. Ready to load data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb884d89-7476-4a20-a513-8ba3846b6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load the DataFrames ---\n",
    "# We use sep='|' because these are pipe-delimited text files.\n",
    "# on_bad_lines='skip' is a safety precaution for these complex files.\n",
    "\n",
    "try:\n",
    "    df_enrollees = pd.read_csv(enrollees_path, sep='|', on_bad_lines='skip')\n",
    "    df_outcomes = pd.read_csv(outcomes_path, sep='|', on_bad_lines='skip')\n",
    "    df_xray_labels = pd.read_csv(xray_labels_path, sep='|', on_bad_lines='skip')\n",
    "    df_clinical = pd.read_csv(clinical_path, sep='|', on_bad_lines='skip')\n",
    "    df_biomarkers = pd.read_csv(biomarkers_path, sep='|', on_bad_lines='skip')\n",
    "\n",
    "    print(\"--- 1. Enrollees (Master List) ---\")\n",
    "    print(f\"Shape: {df_enrollees.shape}\")\n",
    "    print(f\"Columns: {df_enrollees.columns.tolist()}\\n\")\n",
    "\n",
    "    print(\"--- 2. Outcomes (Survival Target) ---\")\n",
    "    print(f\"Shape: {df_outcomes.shape}\")\n",
    "    print(f\"Columns: {df_outcomes.columns.tolist()}\\n\")\n",
    "\n",
    "    print(\"--- 3. X-Ray Labels (Modality 1) ---\")\n",
    "    print(f\"Shape: {df_xray_labels.shape}\")\n",
    "    print(f\"Columns: {df_xray_labels.columns.tolist()}\\n\")\n",
    "    \n",
    "    print(\"--- 4. Clinical Features (Modality 2) ---\")\n",
    "    print(f\"Shape: {df_clinical.shape}\")\n",
    "    print(f\"Columns: {df_clinical.columns.tolist()}\\n\")\n",
    "    \n",
    "    print(\"--- 5. Biomarkers (Modality 3) ---\")\n",
    "    print(f\"Shape: {df_biomarkers.shape}\")\n",
    "    print(f\"Columns: {df_biomarkers.columns.tolist()}\\n\")\n",
    "    \n",
    "    print(\"\\nAll files loaded successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please check the file paths and ensure the files are not empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ddda9c-ba52-49cc-9066-3cdb7ea7a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Standardize Keys & Inspect Key Columns ---\n",
    "\n",
    "# 1. Standardize the Patient ID in the Outcomes table\n",
    "# We rename 'id' (lowercase) to 'ID' (uppercase) to match the other tables\n",
    "try:\n",
    "    df_outcomes.rename(columns={'id': 'ID'}, inplace=True)\n",
    "    print(\"Standardized 'id' -> 'ID' in df_outcomes.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error renaming column: {e}\")\n",
    "\n",
    "\n",
    "# 2. Inspect X-Ray 'SIDE' column\n",
    "# We need to know how \"Right\" and \"Left\" are coded. (Likely 1=Right, 2=Left)\n",
    "print(\"--- Inspecting X-Ray 'SIDE' ---\")\n",
    "if 'SIDE' in df_xray_labels.columns:\n",
    "    print(f\"Values: {df_xray_labels['SIDE'].unique()}\")\n",
    "    print(f\"Value Counts:\\n{df_xray_labels['SIDE'].value_counts()}\\n\")\n",
    "else:\n",
    "    print(\"Column 'SIDE' not found in df_xray_labels!\\n\")\n",
    "\n",
    "\n",
    "# 3. Inspect X-Ray 'V00XRKL' (Baseline KL Grade) column\n",
    "# This is our primary image feature. We need to see the grades.\n",
    "# 0=Healthy, 1=Doubtful, 2=Minimal, 3=Moderate, 4=Severe\n",
    "print(\"--- Inspecting 'V00XRKL' (Baseline KL Grade) ---\")\n",
    "if 'V00XRKL' in df_xray_labels.columns:\n",
    "    print(f\"Values: {df_xray_labels['V00XRKL'].unique()}\")\n",
    "    print(f\"Value Counts (sorted):\\n{df_xray_labels['V00XRKL'].value_counts().sort_index()}\\n\")\n",
    "else:\n",
    "    print(\"Column 'V00XRKL' not found in df_xray_labels!\\n\")\n",
    "\n",
    "# 4. Inspect X-Ray 'READPRJ' (Reading Project) column\n",
    "# 16k rows means multiple reads. We need to pick one.\n",
    "print(\"--- Inspecting 'READPRJ' (Reader Project) ---\")\n",
    "if 'READPRJ' in df_xray_labels.columns:\n",
    "    print(f\"Values: {df_xray_labels['READPRJ'].unique()}\")\n",
    "    print(f\"Value Counts:\\n{df_xray_labels['READPRJ'].value_counts()}\\n\")\n",
    "else:\n",
    "    print(\"Column 'READPRJ' not found in df_xray_labels!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161435d4-2831-4007-a015-d14c54dc2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Clean the X-Ray Labels DataFrame ---\n",
    "\n",
    "# 1. Create a copy to avoid modifying the original\n",
    "df_xray_labels_clean = df_xray_labels.copy()\n",
    "\n",
    "# 2. Filter for the main reading project (READPRJ == 15)\n",
    "df_xray_labels_clean = df_xray_labels_clean[df_xray_labels_clean['READPRJ'] == 15].copy()\n",
    "print(f\"Filtered by READPRJ=15. New shape: {df_xray_labels_clean.shape}\")\n",
    "\n",
    "# 3. Clean 'V00XRKL' (KL Grade)\n",
    "#    - First, replace the 'missing' string with NaN\n",
    "df_xray_labels_clean['V00XRKL'] = df_xray_labels_clean['V00XRKL'].replace(\n",
    "    '.: Missing Form/Incomplete Workbook', np.nan\n",
    ")\n",
    "#    - Second, extract the number (e.g., '2: 2' -> 2).\n",
    "#      We split on ':' and take the first character, then convert to numeric.\n",
    "df_xray_labels_clean['KL_Grade'] = pd.to_numeric(\n",
    "    df_xray_labels_clean['V00XRKL'].str.split(':').str[0]\n",
    ")\n",
    "\n",
    "# 4. Clean 'SIDE'\n",
    "#    - Extract the number (e.g., '1: Right' -> 1)\n",
    "df_xray_labels_clean['Knee_Side'] = pd.to_numeric(\n",
    "    df_xray_labels_clean['SIDE'].str.split(':').str[0]\n",
    ")\n",
    "\n",
    "# 5. Keep only the essential columns\n",
    "df_xray_labels_clean = df_xray_labels_clean[['ID', 'Knee_Side', 'KL_Grade']]\n",
    "\n",
    "# 6. Check for duplicates (e.g., two 'Right' knees for the same patient ID)\n",
    "duplicates = df_xray_labels_clean.duplicated(subset=['ID', 'Knee_Side']).sum()\n",
    "print(f\"Found {duplicates} duplicate knee entries.\\n\")\n",
    "\n",
    "# 7. (If duplicates exist, drop them)\n",
    "if duplicates > 0:\n",
    "    df_xray_labels_clean = df_xray_labels_clean.drop_duplicates(\n",
    "        subset=['ID', 'Knee_Side'], keep='first'\n",
    "    )\n",
    "    print(f\"Dropped duplicates. New shape: {df_xray_labels_clean.shape}\\n\")\n",
    "\n",
    "# 8. Inspect the final clean DataFrame\n",
    "print(\"--- Cleaned X-Ray Labels Info ---\")\n",
    "df_xray_labels_clean.info()\n",
    "print(\"\\n--- Cleaned X-Ray Labels Head ---\")\n",
    "print(df_xray_labels_clean.head())\n",
    "print(\"\\n--- New 'KL_Grade' Value Counts ---\")\n",
    "print(df_xray_labels_clean['KL_Grade'].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c146a57-06ba-406a-958b-88f61ef314c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Create the Master Patient DataFrame ---\n",
    "\n",
    "# 1. Start with the master list of patients\n",
    "df_master_patient = df_enrollees\n",
    "\n",
    "# 2. Merge outcomes (our survival target)\n",
    "# We add suffixes to identify duplicate columns (e.g., VERSION_enrol, VERSION_out)\n",
    "df_master_patient = pd.merge(\n",
    "    df_master_patient, \n",
    "    df_outcomes, \n",
    "    on='ID', \n",
    "    how='outer',\n",
    "    suffixes=('_enrol', '_out')\n",
    ")\n",
    "\n",
    "# 3. Merge clinical features (Modality 2)\n",
    "df_master_patient = pd.merge(\n",
    "    df_master_patient,\n",
    "    df_clinical,\n",
    "    on='ID',\n",
    "    how='outer',\n",
    "    suffixes=('_left', '_clin') # Suffixes from previous merge are handled\n",
    ")\n",
    "\n",
    "# 4. Merge biomarker features (Modality 3)\n",
    "df_master_patient = pd.merge(\n",
    "    df_master_patient,\n",
    "    df_biomarkers,\n",
    "    on='ID',\n",
    "    how='outer',\n",
    "    suffixes=('_clin', '_bio') # Suffixes from previous merge are handled\n",
    ")\n",
    "\n",
    "# 5. Inspect the final master patient table\n",
    "print(\"--- Master Patient DataFrame Info ---\")\n",
    "print(f\"Shape: {df_master_patient.shape}\")\n",
    "print(f\"Total Patients: {df_master_patient['ID'].nunique()}\\n\")\n",
    "\n",
    "# Check for any merge failures (should be 4796)\n",
    "if df_master_patient.shape[0] != 4796:\n",
    "    print(f\"WARNING: Merge created {df_master_patient.shape[0]} rows, not 4796. Check for one-to-many joins.\")\n",
    "else:\n",
    "    print(\"Merge successful: 4796 patients, one row per patient.\")\n",
    "\n",
    "print(\"\\n--- Master Patient DataFrame Head ---\")\n",
    "print(df_master_patient.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bd011-e366-4d8d-a6f5-46deaa26d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Create the Final Tri-Modal Cohort (Knee-Level) ---\n",
    "\n",
    "# We perform a 'left' merge, starting from our clean knee table (df_xray_labels_clean)\n",
    "# and joining the patient-level data (df_master_patient) to it.\n",
    "# This ensures we only keep the 8982 knees we have baseline X-ray data for.\n",
    "\n",
    "df_final_cohort = pd.merge(\n",
    "    df_xray_labels_clean,\n",
    "    df_master_patient,\n",
    "    on='ID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# --- Inspect the Final Cohort DataFrame ---\n",
    "print(\"--- Final Tri-Modal Cohort Info ---\")\n",
    "print(f\"Shape: {df_final_cohort.shape}\")\n",
    "print(f\"Total Knees: {df_final_cohort.shape[0]}\")\n",
    "print(f\"Total Unique Patients: {df_final_cohort['ID'].nunique()}\\n\")\n",
    "\n",
    "# Verify the merge\n",
    "# The shape should be (8982 rows, 3 + 1393 - 1 = 1395 columns)\n",
    "expected_cols = df_xray_labels_clean.shape[1] + df_master_patient.shape[1] - 1\n",
    "if df_final_cohort.shape[0] == 8982 and df_final_cohort.shape[1] == expected_cols:\n",
    "    print(\"SUCCESS: Final cohort created.\")\n",
    "    print(\"Each row now represents one knee and contains all 3 modalities + outcome data.\")\n",
    "else:\n",
    "    print(f\"WARNING: Merge failed. Expected shape (8982, {expected_cols}), got {df_final_cohort.shape}\")\n",
    "\n",
    "print(\"\\n--- Final Cohort DataFrame Head ---\")\n",
    "# This head() will be very wide, but it confirms the merge\n",
    "print(df_final_cohort.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611efe3-fa24-4912-bd79-191607223534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Save the Final Cohort to a File ---\n",
    "\n",
    "# Define a new directory for our processed data\n",
    "import os\n",
    "PROCESSED_DATA_DIR = '../data/processed'\n",
    "if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "    os.makedirs(PROCESSED_DATA_DIR)\n",
    "\n",
    "# Define the output file path\n",
    "OUTPUT_FILE_PATH = f\"{PROCESSED_DATA_DIR}/OAI_tri_modal_cohort_knee_level.parquet\"\n",
    "\n",
    "try:\n",
    "    # Save the DataFrame to Parquet format\n",
    "    # This is much more efficient than CSV for large tables.\n",
    "    df_final_cohort.to_parquet(OUTPUT_FILE_PATH, index=False)\n",
    "    \n",
    "    print(f\"SUCCESS: Final cohort saved to:\")\n",
    "    print(OUTPUT_FILE_PATH)\n",
    "    \n",
    "    # Verify by reloading it\n",
    "    df_reloaded = pd.read_parquet(OUTPUT_FILE_PATH)\n",
    "    print(f\"\\nVerification: Reloaded file with shape {df_reloaded.shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while saving: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbde84-c0a9-4cb5-9c14-12b2379cb861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
